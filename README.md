# GPT2
Introduction:
The goal of GPT-2 is to solve the problem of analyzing language whether for translation, text summarization, or text generation.

This problem is a challenging and important problem in deep learning which there are 
existing solutions but they don't produce satisfactory results.

GPT-2 - Generative Pre-trained Transformer 2 is an open-source artificial intelligence created by OpenAI in February 2019. 

Pre Proccess:
•	Loading Pre Trained Arabic and English model from Transformers.
•	Preparing suitable data set for a specific task.
We found a data set on Harry Potter's books and poets in Arabic.
Training(fine tune):
•	Training the pre trained model on our data sets
•	For each epoch:
Calculate the backward propagation and adjusting better weights
The parameters we used for the training:
Epochs: 5
Batch size: 15
